# -*- coding: utf-8 -*-
"""youtube-api-data-fetcher.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QWbKxzcc-WTstRA4tfx9cGmKeAmL_VZQ
"""

# importing libraries
import pandas as pd      # This imports the pandas library and gives it an alias 'pd'.
                         # Pandas is used for working with tabular data (like spreadsheets).
                         # It helps create, manipulate, and analyze data using data structures like DataFrames.

import requests           # This imports the 'requests' library which allows your code to make HTTP requests.
                         # You’ll use this to connect to web APIs (in this case, YouTube’s API) and get data from the internet.

import time               # This imports Python’s built-in 'time' module.
                         # It allows you to pause or delay the execution of your code.
                         # Useful when you want to avoid overwhelming the API by sending too many requests too quickly (called “rate limiting”).

API_KEY = "AIzaSyBTzRCR3fUIe4ktLg2qgvh87K4YeAEAd10"  # Your personal YouTube Data API key used to authenticate requests
CHANEL_ID = "UCIvFU5ctIISwVQzsXE7__lw"              # The unique ID of the YouTube channel you want to get videos from

# making api call
pageToken = ""  # Empty string for pageToken (used for pagination if there are many videos); can be updated to get the next page of results

url = "https://www.googleapis.com/youtube/v3/search?key=" + API_KEY + \
      "&channelId=" + CHANEL_ID + \
      "&part=snippet,id&order=date&maxResults=10000" + pageToken
      # This constructs the full URL to call the YouTube API 'search' endpoint
      # key=API_KEY: authenticates you
      # channelId=CHANEL_ID: specifies which channel you're pulling videos from
      # part=snippet,id: asks for metadata (title, date, etc.) and video ID
      # order=date: fetches videos sorted by newest first
      # maxResults=10000: sets the max number of results (should be ≤50, 10000 is invalid and will be ignored by the API)
      # + pageToken: appends the pagination token if you're requesting next pages (currently blank)

response = requests.get(url).json()  # Sends an HTTP GET request to the URL, receives the API response, and parses it to a Python dictionary (JSON)

response  # Shows the entire JSON response, which contains video details like title, ID, and date

response['items']  # Accesses the list of videos and other items returned by the API; each item contains details like title, video ID, and publish date

response['items'][0]  # Retrieves the first item (i.e., the most recent video or content) in the list of returned search results

video_id = response['items'][0]['id']['videoId']  # Extracts the 'videoId' from the 'id' section of the first video item in the API response
video_id  # Displays the extracted video ID (a unique string that identifies the video on YouTube)

video_title = response['items'][0]['snippet']['title']  # Extracts the title of the first video from the 'snippet' section
video_title  # Displays the video title (usually a string like the name of the video on YouTube)

upload_date = response['items'][0]['snippet']['publishedAt']  # Gets the full publish date and time of the first video (in ISO 8601 format, e.g., '2024-05-09T15:00:00Z')
upload_date = str(upload_date).split("T")[0]  # Converts the datetime string to just the date by splitting at 'T' and keeping the first part (e.g., '2024-05-09')
upload_date  # Displays the cleaned upload date as a string in 'YYYY-MM-DD' format

def get_video_details(video_id):  # Define a function that takes a video_id and returns its stats

    # second API call to get video statistics (views, likes, comments)
    url_video_stats = "https://www.googleapis.com/youtube/v3/videos?id=" + video_id + "&part=statistics&key=" + API_KEY
    # Construct the URL to fetch statistics for a single video using its video ID and API key

    response_video_stats = requests.get(url_video_stats).json()
    # Send the HTTP GET request to the API and convert the response into a JSON format (dictionary)

    view_count = response_video_stats['items'][0]['statistics']['viewCount']
    # Extract the number of views for the video

    like_count = response_video_stats['items'][0]['statistics']['likeCount']
    # Extract the number of likes for the video

    comment_count = response_video_stats['items'][0]['statistics']['commentCount']
    # Extract the number of comments for the video

    return view_count, like_count, comment_count  # Return the three stats as a tuple

def get_videos(df):  # Define a function that takes an existing DataFrame and adds video data to it

    # making API call
    pageToken = ""  # Page token is used for pagination; currently empty so it fetches the first page
    url = "https://www.googleapis.com/youtube/v3/search?key=" + API_KEY + "&channelId=" + CHANEL_ID + "&part=snippet,id&order=date&maxResults=50" + pageToken
    # Construct the API URL to fetch video metadata from a specific channel (latest 50 videos)

    response = requests.get(url).json()  # Send GET request and convert the response to a JSON object (dictionary)

    time.sleep(1)  # Pause for 1 second to avoid hitting API rate limits

    for video in response['items']:  # Loop through each item (video or playlist or other) in the response
        if video['id']['kind'] == 'youtube#video':  # Ensure we're only processing actual videos (not playlists or channels)

            video_id = video['id']['videoId']  # Extract the unique ID of the video
            video_title = video['snippet']['title']  # Get the title of the video
            upload_date = video['snippet']['publishedAt']  # Get the full date-time the video was published
            upload_date = str(upload_date).split("T")[0]  # Extract just the date (YYYY-MM-DD)

            view_count, like_count, comment_count = get_video_details(video_id)
            # Call a separate function to get video statistics: views, likes, comments

            df = pd.concat([df, pd.DataFrame([{
                'video_id': video_id,  # Add video ID to the new row
                'video_title': video_title,  # Add title
                'upload_date': upload_date,  # Add upload date
                'view_count': view_count,  # Add view count
                'like_count': like_count,  # Add like count
                'comment_count': comment_count  # Add comment count
            }])], ignore_index=True)  # Append the new row to the DataFrame

    return df  # Return the updated DataFrame with all video info

# saving in pandas dataframe
df = pd.DataFrame(columns=["video_id", "video_title", "upload_date", "view_count", "like_count", "comment_count"])
# Create an empty DataFrame with specific column names to hold video data

df = get_videos(df)
# Call the get_videos function you defined earlier
# It will fill the DataFrame by fetching video metadata and statistics using the YouTube API

df
# Display the filled DataFrame (shows all collected video information)